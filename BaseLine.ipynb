{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dukhyun\\AppData\\Local\\conda\\conda\\envs\\keras\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")  # window에서 Mecab을 사용하기 위해 설정 후 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "with codecs.open('Citation_Data.txt', 'r', 'utf-8') as f:\n",
    "    docs = f.read()\n",
    "bs = BeautifulSoup(docs, \"lxml\")\n",
    "docsList = bs.find_all('c')\n",
    "del docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터 정리\n",
    "docs = []\n",
    "for doc in docsList:\n",
    "    cited = doc.get('citedarticle')\n",
    "    citing = doc.get('citingarticle')\n",
    "    clue = doc.get('clue')\n",
    "    sentiment = doc.get('sentiment')\n",
    "    sentiments = [str(l.text) for l in doc.find_all('s')] # l.text로 인해 target tag 역시 사라집니다.\n",
    "    docs.append([cited, citing, sentiment, clue, ' '.join(sentiments)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clue, Sentiments, Sentiment로 데이터 재정리\n",
    "pnn = {'POS':2, 'NEG':1, 'NEU':0, 'NEU,NEU':0}\n",
    "X = np.array([l[4] for l in docs])\n",
    "Y = np.array([pnn[l[2]] for l in docs])\n",
    "clue = np.array([l[3] for l in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터를 4/5는 train set, 1/5는 test set으로 이용합니다.(비율을 맞춰 나눕니다.)\n",
    "# 우선 반복을 5회 하지는 않습니다.\n",
    "skf = StratifiedKFold(Y, n_folds=5, shuffle=True)\n",
    "\n",
    "for train, test in skf:\n",
    "    trainX, trainY, trainClue = X[train], Y[train], clue[train]\n",
    "    testX, testY, testClue = X[test], Y[test], clue[test]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Clue를 단어 Feature로 사용하기 위하여 형태소 분석을하여 단어 사전을 구축합니다.\n",
    "# 형태소 분석기는 Mecab을 이용합니다.\n",
    "word_dict = []\n",
    "if True:  # True로 하면 Clue만을 이용하여 word_dict를 구성합니다.\n",
    "    for c in trainClue:\n",
    "        if c:\n",
    "            word_dict += [i[0]+'/'+i[1] for i in mecab.pos(c.replace(' ', ''))]\n",
    "\n",
    "    word_dict = set(word_dict)\n",
    "    word_dict = {w:i for i, w in enumerate(word_dict)}\n",
    "elif True: # 위에서 False일때이며, 문장에서 단어를 뽑아내 feature로 사용합니다.\n",
    "    word_dict = []\n",
    "    for doc in trainX:\n",
    "        word_dict += [i[0]+'/'+i[1] for i in mecab.pos(doc.replace(' ', '')) if i[1] != 'SY']\n",
    "    word_dict = set(word_dict)\n",
    "    word_dict = {w:i for i, w in enumerate(word_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 넣을 공간을 만듭니다.\n",
    "train_X = np.zeros((len(trainY), len(word_dict)), dtype=np.int32)\n",
    "test_X = np.zeros((len(testY), len(word_dict)), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터들의 문장을 띄어쓰기를 제거하고 형태소 분석하고, 기타 기호 SY 형태소를 제거하였음\n",
    "# 띄어쓰기를 제거한 이유는 다음과 같습니다.\n",
    "# 문장을 읽어올때 잘못된 띄어쓰기 문제가 발생하였기 때문에 띄어쓰기를 제거하여 형태소 분석을 합니다. \n",
    "# Train Set\n",
    "for i, doc in enumerate(trainX):\n",
    "    doc = [i[0]+'/'+i[1] for i in mecab.pos(doc.replace(' ', '')) if i[1] != 'SY']\n",
    "    for w in doc:\n",
    "        try:\n",
    "            train_X[word_dict[w], i] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Test Set\n",
    "for i, doc in enumerate(testX):\n",
    "    doc = [i[0]+'/'+i[1] for i in mecab.pos(doc.replace(' ', '')) if i[1] != 'SY']\n",
    "    for w in doc:\n",
    "        try:\n",
    "            test_X[word_dict[w], i] += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.26371681415929205\n",
      "accuracy:  0.3591549295774648\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NEU       0.78      0.31      0.44       101\n",
      "        NEG       0.27      0.20      0.23        15\n",
      "        POS       0.19      0.65      0.29        26\n",
      "\n",
      "avg / total       0.61      0.36      0.39       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NaiveBayes\n",
    "clf = GaussianNB()\n",
    "clf.fit(train_X, trainY)\n",
    "print(\"train accuracy: \", len(trainY[clf.predict(train_X) == trainY]) / len(trainY))\n",
    "print(\"accuracy: \", len(testY[clf.predict(test_X) == testY]) / len(testY))\n",
    "print(\"\\n\", classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.8601769911504424\n",
      "accuracy:  0.5774647887323944\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NEU       0.69      0.78      0.73       101\n",
      "        NEG       0.13      0.13      0.13        15\n",
      "        POS       0.08      0.04      0.05        26\n",
      "\n",
      "avg / total       0.52      0.58      0.55       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "clf = LinearSVC()\n",
    "clf.fit(train_X, trainY)\n",
    "print(\"train accuracy: \", len(trainY[clf.predict(train_X) == trainY]) / len(trainY))\n",
    "print(\"accuracy: \", len(testY[clf.predict(test_X) == testY]) / len(testY))\n",
    "print(\"\\n\", classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:  0.8265486725663717\n",
      "accuracy:  0.676056338028169\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NEU       0.71      0.93      0.80       101\n",
      "        NEG       0.50      0.07      0.12        15\n",
      "        POS       0.14      0.04      0.06        26\n",
      "\n",
      "avg / total       0.58      0.68      0.59       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_X, trainY)\n",
    "print(\"train accuracy: \", len(trainY[clf.predict(train_X) == trainY]) / len(trainY))\n",
    "print(\"accuracy: \", len(testY[clf.predict(test_X) == testY]) / len(testY))\n",
    "print(\"\\n\", classification_report(testY, clf.predict(test_X), target_names=['NEU', 'NEG', 'POS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
